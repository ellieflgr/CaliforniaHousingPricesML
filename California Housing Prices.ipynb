{"metadata":{"orig_nbformat":4,"celltoolbar":"Raw Cell Format","kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\n!{sys.executable} -m pip install pandas matplotlib zlib","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport tarfile\nimport urllib\n\nDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n\n#calling this function creates a datasets/housing directory in my workspace,downloads the\n#housing.tgz file and from that extracts the housing.tgz file in this directory\n\ndef fetch_housing_data(housing_url = HOUSING_URL, housing_path = HOUSING_PATH):\n    os.makedirs(housing_path, exist_ok = True)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path = housing_path)\n    housing_tgz.close()\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fetch_housing_data()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef load_housing_data(housing_path = HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing = load_housing_data()\nhousing.head() \n#gives te top five rows of the pandas data frame object\n#each row represents one district\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Provides general information about the dataset, total number of rows (amount of districts), type of each\n#attribute and the number of non null values. Note that the value \"amount of total bedrooms\" is null in\n#207 districts.\nhousing.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shows which types of values there are in the housing data and how many districts belong to each category\nhousing[\"ocean_proximity\"].value_counts()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* *count* decribes the count of values in each attribute (null values are ignored) <br />\n* *std* = standard deviation/ dispersion of values <br />\n* *percentiles* indicate the value below which a given percentage in a group of observations fall,\n     example: 25% of the districts have a housing median age below 18, 50 % below 29 and so on. It is \n     referred to as first quartile (25th percentile), median (50th), third quartile (75th).\n\n\n\n","metadata":{}},{"cell_type":"code","source":"%matplotlib inline \n##uses jupyters backend\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show() #optional","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Oberservations from plots:** <br/>\n* housing median ange, median income and median house value are preprocessed. <br />\n* median income is scaled, value = value * tens of thousands -> 6 equals 60.000 USD <br />\n* median house value was capped at 500k, ML algorithm might learn that house values do not go over that price --> problem, as this is the target value of our problem. <br />\n* median income also seems to be capped <br />\n* differences in scaling <br />\n* tail heavy - histograms extend more to the right of the median <br />\n* avoid snooping bias from first impressions! <br/>","metadata":{}},{"cell_type":"markdown","source":"<h3>Creating a test set</h3>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n#function for splitting data into train set and test set \ndef split_train_test(data, test_ratio): \n    shuffled_indices = np.random.permutation(len(data))\n    #test set size = 20 per cent of dataset size\n    test_set_size = int(len(data) * test_ratio) \n    \n    test_indices = shuffled_indices[:test_set_size] #first 80% of shuffled indices\n    train_indices = shuffled_indices[test_set_size:] #remaining 20% of shuffled indices\n    return data.iloc[train_indices], data.iloc[test_indices]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set, test_set = split_train_test(housing, 0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_set)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_set)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**These two solutions are not stable as when we run the program again it will create a different train and test split and data that was in train before will go into test and vice versa.**\n\n**Possible \"solutions\":**\n* save test set on first run and then load it in subsequent runs\n* setting the seed of np.random.permutation so that it always generates the same shuffled indices\n\nBut also these will break if we want to fetch an updated dataset. We want to have a stable train/test split that remains stable also when updating the data.\n\n**A common solution for this issue:**\n* use each instances identifier to decide whether or not they should go in the test set (assuming that each instance has a unique and immutable identifier\n* we have the possibility of computing a hash of each instances identifier and decide whether if goes into train or test based on the hash being f.e. higher or lower (lower/equal) than 20% of the max hash value.\n* this ensures that even if we refresh the data set the new test set will contain 20% of the new instances while also keeping it guaranteed that\n    * the \"old\" values stay in the test set\n    * 20% of the new values are in the test set\n    * no value from the train set will go into the test set\n    \nThis is what an implementation of that principle looks like:","metadata":{}},{"cell_type":"code","source":"from zlib import crc32\n\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_ny_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the housing dataset does not have an identifier column, simple solution: using row index as id \n#we have to always ensure that new data always gets appended to the end of the dataset and no row gets deleted\n\nhousing_with_id = housing.reset_index() #adding index column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#alternative: using most stable features to build unique identifier\n#f.e. longitude/latitude will remain stable; we can combine them into a unique id like so:\n\nhousing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
